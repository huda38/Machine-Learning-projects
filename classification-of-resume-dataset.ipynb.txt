{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"imports\"></a>\n# Imports\n\nfollowing libraries are used for:\n1. re - remove unwanted charater from string using regulare expression\n2. nltk - remove stop words and conjunctions\n3. numpy - transform data into respective shape\n4. pandas - import data from file into dataframe\n5. matplotlib - create visualization\n6. wordcloud - create word cloud","metadata":{}},{"cell_type":"markdown","source":"# **Resume Screening**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\nnltk.download(\"stopwords\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"theme-configuration\"></a>\n# Theme Configuration\n\nchoosing color template for graph and word cloud","metadata":{}},{"cell_type":"code","source":"# for other theme, please run: mpl.pyplot.style.available\nPLOT_PALETTE = 'tableau-colorblind10'\n# for other color map, please run: mpl.pyplot.colormaps()\nWORDCLOUD_COLOR_MAP = 'tab10_r'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set palette color\nplt.style.use(PLOT_PALETTE)\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"read-csv\"></a>\n# Read CSV\n\nID and Resume_html columns does not contain useful information and are not aligend with my interest. Therefore, both columns are removed.","metadata":{}},{"cell_type":"code","source":"#df = pd.read_csv('../input/resume-dataset/Resume/Resume.csv')\ndf = pd.read_csv('/kaggle/input/d/snehaanbhawal/resume-dataset/Resume/Resume.csv')\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop unused columns\ndel df['ID']\ndel df['Resume_html']\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"preprocessing\"></a>\n# Preprocessing\n\nIn this stage, I followed basic cleaning processes for text analysis which includes:\n1. converting characters to lowercases.\n2. remove punctuations, numbers and foreign languages.\n3. tokenize word. (spliting sentence into unigram)\n","metadata":{}},{"cell_type":"code","source":"def preprocess(txt):\n    # convert all characters in the string to lower case\n    txt = txt.lower()\n    # remove non-english characters, punctuation and numbers\n    txt = re.sub('[^a-zA-Z]', ' ', txt)\n    txt = re.sub('http\\S+\\s*', ' ', txt)  # remove URLs\n    txt = re.sub('RT|cc', ' ', txt)  # remove RT and cc\n    txt = re.sub('#\\S+', '', txt)  # remove hashtags\n    txt = re.sub('@\\S+', '  ', txt)  # remove mentions\n    txt = re.sub('\\s+', ' ', txt)  # remove extra whitespace\n    # tokenize word\n    txt = nltk.tokenize.word_tokenize(txt)\n    # remove stop words\n    txt = [w for w in txt if not w in nltk.corpus.stopwords.words('english')]\n    \n\n    return ' '.join(txt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessing text\ndf['Resume_str'] = df['Resume_str'].apply(lambda w: preprocess(w))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"exploratory-data-analysis\"></a>\n# Exploratory Data Analysis\n\nEDA is excercised to inspect class imbalance, word similarity and word frequency.","metadata":{}},{"cell_type":"code","source":"import string\nfrom nltk.corpus import stopwords\noneSetOfStopWords = set(stopwords.words('english')+['``',\"''\"])\ntotalWords =[]\nSentences = df['Resume_str'].values\ncleanedSentences = \"\"\nfor records in Sentences:\n    cleanedSentences += records\n    requiredWords = nltk.word_tokenize(records)\n    for word in requiredWords:\n        if word not in oneSetOfStopWords and word not in string.punctuation:\n            totalWords.append(word)\n    \nwordfreqdist = nltk.FreqDist(totalWords)\nmostcommon = wordfreqdist.most_common(50)\nprint(mostcommon)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc = WordCloud().generate(cleanedSentences)\nplt.figure(figsize=(10,10))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create list of all categories\ncategories = np.sort(df['Category'].unique())\ncategories","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create new df for corpus and category\ndf_categories = [df[df['Category'] == category].loc[:, ['Resume_str', 'Category']] for category in categories]\ndf_categories","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"barchart\"></a>\n## Bar Chart\n\nThe bar chart shows the number of records for each category, where class imbalanced is spotted easily.","metadata":{}},{"cell_type":"code","source":"df['Category'].value_counts().sort_index().plot(kind='bar', figsize=(12, 6))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"wordcloud\"></a>\n## Word Cloud\n\nAfter word clouds are created, the word \"manag\" (inflected for \"manage\") prominently visible on many categories. Moreover, words like \"citi\", \"state\" and \"compani\" are noticeable on different categories as well. These common words are likely to contain low weight for computation. On the other hand, words like \"account\", \"develop\" and \"design\" probably has higher weight for calculation because it only appears on specific domains.","metadata":{}},{"cell_type":"code","source":"def wordcloud(df):\n    txt = ' '.join(txt for txt in df['Resume_str'])\n    wordcloud = WordCloud(\n        height=2000,\n        width=4000,\n        colormap=WORDCLOUD_COLOR_MAP\n    ).generate(txt)\n\n    return wordcloud","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(32, 28))\n\nfor i, category in enumerate(categories):\n    wc = wordcloud(df_categories[i])\n\n    plt.subplot(5, 5, i + 1).set_title(category)\n    plt.imshow(wc)\n    plt.axis('off')\n    plt.plot()\n\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"wordfreq\"></a>\n## Word Frequency Table\n\nWord frequency is plotted to visualize how often popular words are used. Most categories have similar distribution on top 10 frequently used words.","metadata":{}},{"cell_type":"code","source":"def wordfreq(df):\n    count = df['Resume_str'].str.split(expand=True).stack().value_counts().reset_index()\n    count.columns = ['Word', 'Frequency']\n\n    return count.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(32, 64))\n\nfor i, category in enumerate(categories):\n    wf = wordfreq(df_categories[i])\n\n    fig.add_subplot(5, 5, i + 1).set_title(category)\n    plt.bar(wf['Word'], wf['Frequency'])\n    plt.ylim(0, 3500)\n\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nvar_mod = ['Category']\nle = LabelEncoder()\nfor i in var_mod:\n    df[i] = le.fit_transform(df[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Category.value_counts()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Remove Category with a few records (implanced data)\n#df = df[df.Category != 2]\n#df = df[df.Category != 5]\n#df = df[df.Category != 8]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import hstack\n\nrequiredText = df['Resume_str'].values\nrequiredTarget = df['Category'].values\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    stop_words='english')\nword_vectorizer.fit(requiredText)\nWordFeatures = word_vectorizer.transform(requiredText)\n\nprint (\"Feature completed .....\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(WordFeatures,requiredTarget,random_state=42, test_size=0.2,\n                                                 shuffle=True, stratify=requiredTarget)\nprint(X_train.shape)\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = OneVsRestClassifier(KNeighborsClassifier())\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nprint('Accuracy of KNeighbors Classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of KNeighbors Classifier on test set:     {:.2f}'.format(clf.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n Classification report for classifier %s:\\n%s\\n\" % (clf, metrics.classification_report(y_test, prediction)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Deep Learning**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/d/snehaanbhawal/resume-dataset/Resume/Resume.csv')\ndf['Resume_str'] = df['Resume_str'].apply(lambda w: preprocess(w))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\n\nfeatures = df['Resume_str'].values\noriginal_labels = df['Category'].values\nlabels = original_labels[:]\n\nfor i in range(len(df)):\n    \n    labels[i] = str(labels[i].lower())  # convert to lowercase\n    labels[i] = labels[i].replace(\" \", \"\")  # use hyphens to convert multi-token labels into single tokens\n    \nfeatures, labels = shuffle(features, labels)\n\n# Print example feature and label\nprint(features[0])\nprint(labels[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split for train and test\ntrain_split = 0.8\ntrain_size = int(train_split * len(df))\n\ntrain_features = features[:train_size]\ntrain_labels = labels[:train_size]\n\ntest_features = features[train_size:]\ntest_labels = labels[train_size:]\n\n# Print size of each split\nprint(len(train_labels))\nprint(len(test_labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n# Tokenize feature data\nvocab_size = 6000\noov_tok = '<>'\n\nfeature_tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\nfeature_tokenizer.fit_on_texts(features)\n\nfeature_index = feature_tokenizer.word_index\nprint(dict(list(feature_index.items())))\n\ntrain_feature_sequences = feature_tokenizer.texts_to_sequences(train_features)\n\ntest_feature_sequences = feature_tokenizer.texts_to_sequences(test_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize label data \nlabel_tokenizer = Tokenizer(lower=True)\nlabel_tokenizer.fit_on_texts(labels)\n\nlabel_index = label_tokenizer.word_index\nprint(dict(list(label_index.items())))\n\ntrain_label_sequences = label_tokenizer.texts_to_sequences(train_labels)\n\ntest_label_sequences = label_tokenizer.texts_to_sequences(test_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pad sequences for feature data\nmax_length = 300\ntrunc_type = 'post'\npad_type = 'post'\n\ntrain_feature_padded = pad_sequences(train_feature_sequences, maxlen=max_length, padding=pad_type, truncating=trunc_type)\ntest_feature_padded = pad_sequences(test_feature_sequences, maxlen=max_length, padding=pad_type, truncating=trunc_type)\n\nprint(train_feature_padded[0])\nprint(test_feature_padded[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the neural network\nembedding_dim = 64\n\nmodel = tf.keras.Sequential([\n  # Add an Embedding layer expecting input vocab of size 6000, and output embedding dimension of size 64 we set at the top\n  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=1),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n  #tf.keras.layers.Dense(embedding_dim, activation='relu'),\n\n  # use ReLU in place of tanh function since they are very good alternatives of each other.\n  tf.keras.layers.Dense(embedding_dim, activation='relu'),\n\n  # Add a Dense layer with 25 units and softmax activation for probability distribution\n  tf.keras.layers.Dense(26, activation='softmax')\n])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the model and convert train/test data into NumPy arrays\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Features\ntrain_feature_padded = np.array(train_feature_padded).astype(np.int32)\ntest_feature_padded = np.array(test_feature_padded).astype(np.int32)\n\n# Labels\ntrain_label_sequences = np.array(train_label_sequences).astype(np.int32)\ntest_label_sequences = np.array(test_label_sequences).astype(np.int32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feature_padded = train_feature_padded.astype(np.float32)\ntrain_label_sequences = train_label_sequences.astype(np.float32)\ntest_feature_padded = test_feature_padded.astype(np.float32)\ntest_label_sequences = test_label_sequences.astype(np.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the neural network\nnum_epochs = 12\n\nhistory = model.fit(train_feature_padded, train_label_sequences, epochs=num_epochs, validation_data=(test_feature_padded, test_label_sequences), verbose=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n# Define the tokenizer\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\n\n# Convert training and testing sequences to sequences of integers\ntrain_sequences = tokenizer.texts_to_sequences(training_sentences)\ntest_sequences = tokenizer.texts_to_sequences(testing_sentences)\n\n# Pad the sequences so that they are all the same length\ntrain_feature_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\ntest_feature_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# Convert the labels to sequences of integers\ntrain_label_sequences = np.array(training_labels)\ntest_label_sequences = np.array(testing_labels)\n\n# Convert the data to NumPy arrays of the appropriate data type\ntrain_feature_padded = train_feature_padded.astype(np.float32)\ntrain_label_sequences = train_label_sequences.astype(np.int32)\ntest_feature_padded = test_feature_padded.astype(np.float32)\ntest_label_sequences = test_label_sequences.astype(np.int32)\n\n# Define the model architecture\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model\nnum_epochs = 30\nhistory = model.fit(train_feature_padded, train_label_sequences, epochs=num_epochs, validation_data=(test_feature_padded, test_label_sequences), verbose=2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n# Convert input data to float32\ntrain_feature_padded = train_feature_padded.astype(np.float32)\ntest_feature_padded = test_feature_padded.astype(np.float32)\ntokenizer = Tokenizer(num_words=5000)\n# Convert label sequences to one-hot encoding\nnum_classes = len(tokenizer.word_index) + 1\ntrain_label_one_hot = tf.keras.utils.to_categorical(train_label_sequences, num_classes=num_classes)\ntest_label_one_hot = tf.keras.utils.to_categorical(test_label_sequences, num_classes=num_classes)\n\n# Train the neural network\nnum_epochs = 12\n\nhistory = model.fit(train_feature_padded, train_label_one_hot, epochs=num_epochs, validation_data=(test_feature_padded, test_label_one_hot), verbose=2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = model.evaluate(test_feature_padded, test_label_sequences, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Test Accuracy:\", score[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\n\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','test'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Resume Parsing**","metadata":{}}]}